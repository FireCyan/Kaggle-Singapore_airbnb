{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "motivated-happiness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, validation_curve, learning_curve\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, cross_validate\n",
    "\n",
    "# For result evaluation\n",
    "from sklearn.metrics import explained_variance_score, max_error, mean_absolute_error, mean_squared_error, median_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "organic-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "df_processed = pd.read_pickle('./processed_data.p')\n",
    "# df_singa_airbnb = pd.read_csv('listings.csv')\n",
    "\n",
    "col_train = [x for x in df_processed.columns if x not in ['name', 'host_name', 'last_review', 'last_review_date', 'price', 'id', 'host_name', 'host_id', 'last_review_year', 'last_review_month', 'last_review_week', 'last_review_day', 'last_review_dayofweek']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "competent-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_processed[col_train].values\n",
    "y = df_processed['price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "mental-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "lyric-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "rural-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "def print_regression_scores(clf, x_train, y_train, x_test, y_test, display = 1, cv = 10):\n",
    "    \n",
    "    dict_result = {}\n",
    "    cross_val = StratifiedShuffleSplit(n_splits = cv, random_state=random_state)\n",
    "\n",
    "    for name, feat, tar in [(\"TRAIN set\", x_train, y_train), (\"TEST set\", x_test, y_test)]:\n",
    "        explained_variance = explained_variance_score(tar, clf.predict(feat))\n",
    "        max_err = max_error(tar, clf.predict(feat))\n",
    "        mean_absolute_err = mean_absolute_error(tar, clf.predict(feat))\n",
    "        mean_squared_err = mean_squared_error(tar, clf.predict(feat))\n",
    "        median_absolute_err = median_absolute_error(tar, clf.predict(feat))\n",
    "        r2 = r2_score(tar, clf.predict(feat))\n",
    "        mean_absolute_percentage_err = mean_absolute_percentage_error(tar, clf.predict(feat))\n",
    "               \n",
    "        if display == 1:\n",
    "            print(\"{} explained variance score: \".format(name), explained_variance)\n",
    "            print(\"{} max error: \".format(name), max_err)\n",
    "            print(\"{} mean absolute error: \".format(name), mean_absolute_err)\n",
    "            print(\"{} mean squared error: \".format(name), mean_squared_err)\n",
    "            print(\"{} median absolute error: \".format(name), median_absolute_err)\n",
    "            print(\"{} r2 score: \".format(name), r2)\n",
    "            \n",
    "            \n",
    "            print(\"{} mean_absolute percentage error : \".format(name), mean_absolute_percentage_err, end = '\\n\\n')\n",
    "        \n",
    "        dict_result[name] = [explained_variance, max_err, mean_absolute_err, mean_squared_err, median_absolute_err, r2, mean_absolute_percentage_err]\n",
    "        \n",
    "    return dict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "greek-douglas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_classification_scores(clf, x_train, y_train, x_test, y_test, display = 1, cv = 10):\n",
    "    \n",
    "#     dict_result = {}\n",
    "#     cross_val = StratifiedShuffleSplit(n_splits = cv, random_state=random_state)\n",
    "\n",
    "#     for name, feat, tar in [(\"TRAIN set\", x_train, y_train), (\"TEST set\", x_test, y_test)]:\n",
    "#         f1_macro = f1_score(tar, clf.predict(feat), average = 'macro')\n",
    "#         recall_macro = recall_score(tar, clf.predict(feat), average = 'macro')\n",
    "#         precision_macro = precision_score(tar, clf.predict(feat), average = 'macro')\n",
    "#         balanced_accuracy = balanced_accuracy_score(tar, clf.predict(feat))\n",
    "               \n",
    "#         if display == 1:\n",
    "#             print(\"{} balanced_accuracy score: \".format(name), balanced_accuracy)\n",
    "#             print(\"{} recall score: \".format(name), recall_macro)\n",
    "#             print(\"{} precision score: \".format(name), precision_macro)\n",
    "#             print(\"{} f1_macro score: \".format(name), f1_macro, end = '\\n\\n')\n",
    "        \n",
    "#         dict_result[name] = [balanced_accuracy, recall_macro, precision_macro, f1_macro]\n",
    "        \n",
    "#     return dict_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-newsletter",
   "metadata": {},
   "source": [
    "### 1. Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fancy-antique",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04294767783294129"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "reg = lr.fit(X,y)\n",
    "reg.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "elegant-imaging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN set explained variance score:  0.037464461240404145\n",
      "TRAIN set max error:  9874.52927377299\n",
      "TRAIN set mean absolute error:  92.67884313652527\n",
      "TRAIN set mean squared error:  117565.89902415503\n",
      "TRAIN set median absolute error:  59.997901123606965\n",
      "TRAIN set r2 score:  0.037464386691688945\n",
      "TRAIN set mean_absolute percentage error :  186736630576594.03\n",
      "\n",
      "TEST set explained variance score:  0.07267511305487862\n",
      "TEST set max error:  8556.0017625842\n",
      "TEST set mean absolute error:  91.91424081082708\n",
      "TEST set mean squared error:  83466.15824051782\n",
      "TEST set median absolute error:  61.13396362731568\n",
      "TEST set r2 score:  0.072673495961591\n",
      "TEST set mean_absolute percentage error :  0.7243692716185841\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dict_result = print_regression_scores(lr, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "charitable-scholar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.48362503e-07,  1.50400679e-08,  1.71512606e+01,  1.51974121e+00,\n",
       "       -2.24830801e+02, -2.06692233e+01, -1.13936995e+02, -1.68196831e+00,\n",
       "       -6.33846903e+00,  1.08030983e+01, -8.89347086e+01,  3.20033677e+01,\n",
       "       -5.66622934e+00, -2.27182539e+00,  3.81584331e-01,  1.92736699e+00,\n",
       "        9.87894419e-01])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dirty-sentence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.90910000e+04, 2.66763000e+05, 2.00000000e+00, ...,\n",
       "        4.30000000e+01, 2.10000000e+01, 0.00000000e+00],\n",
       "       [5.06460000e+04, 2.27796000e+05, 0.00000000e+00, ...,\n",
       "        5.20000000e+01, 2.60000000e+01, 4.00000000e+00],\n",
       "       [5.63340000e+04, 2.66763000e+05, 2.00000000e+00, ...,\n",
       "        4.00000000e+01, 1.00000000e+00, 3.00000000e+00],\n",
       "       ...,\n",
       "       [3.81093360e+07, 2.81448565e+08, 0.00000000e+00, ...,\n",
       "        2.60000000e+01, 2.70000000e+01, 3.00000000e+00],\n",
       "       [3.81104930e+07, 2.43835202e+08, 0.00000000e+00, ...,\n",
       "        2.60000000e+01, 2.70000000e+01, 3.00000000e+00],\n",
       "       [3.81127620e+07, 2.87885200e+07, 0.00000000e+00, ...,\n",
       "        2.60000000e+01, 2.70000000e+01, 3.00000000e+00]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-profession",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
